## MVP
Я реализовал простую модель, она запускается и обучается, но к сожалению обучить её времени не было из-за загруженности по учёбе.

#### Данные
Возьмем из каждой секунды видео один кадр, например первый. 
Каждому из этих кадров сопоставим метку $0$ или $1$ — индикатор того, входит ли текущий кадр(секунда) в заставку. 
Далее создадим датасет, в котором в рамках одного батча может встречаться только одно видео. 
Эти данные будут использоваться для обучения и валидации модели.
Функции для этого этапа находятся в `preprocess.py`.

#### Модель
Для учета временной структуры будем использовать RNN (у меня это Bidirectional LSTM). В качестве признаков возьмем векторы эмбедингов, которые получим из CNN (у меня это ResNet50).
Архитектура модели описана в `model.py`.

#### Обучение
В файле `train.py` написан стандартный цикл обучения и подсчет метрик.

#### Оценка качества
Так как заставка занимает малую часть видео будем использовать Precision, Recall и F1 меру.

#### Почему это работает

[В данной статье](https://www.amazon.science/publications/intro-and-recap-detection-for-movies-and-tv-series) описан такой метод, только с использованием ещё аудио-признаков и CRF для сглаживания предсказний.


## Модификации

#### Данные
Можем попробовать увеличить число кадров, которые используем в модели или как-то агрегировать их, например использовать пулинг (может быть плохо, если заставки бывают неоднородными, например часто меняются цвета).

#### Модель
- Попробовать разные энкодеры, например из моделей, которые уже используются в других задачах, требующих извлечение признаков из видео-кадров, или из классических CV моделей.

- [В статье 2025 года](https://arxiv.org/abs/2504.09738) авторы предлагают заменить CNN на энкодер изображений из CLIP, а вместо RNN использовать Multihead Attention (с positional encoder). 
Отмечается, что метод дает значительный прирост метрик по сравнению с RNN+CNN, так же приведены сравнения разных энкодеров.

- Попробовать заменить RNN/Attention на SMM (State Space Models). 
Из плюсов SMM можно выделить возможность использовать больший контекст за счёт линейной сложности от длины контекста (у слоя внимания квадратичная, но наверное для нашей задачи это не так критично), относительно RNN они меньше подвержены взрыву/затуханию градиентов, можно параллелить. 
На сколько я знаю в реальных задачах SSM редко бъют трансформеры, но попробовать всегда можно.